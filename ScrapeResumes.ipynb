{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scrape Resumes</h1>\n",
    "<h2>Be careful not to scrape too much</h2>\n",
    "The resumes on Indeed are fairly well structured in their html. I want to scrape the job titles and see what signal I have to work with for measuring how often job transitions have occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium  import webdriver\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def get_soup(start=0):\n",
    "    '''\n",
    "    This function loads the resumes database at Indeed.com for new york starting at item number start\n",
    "    It then scrapes the page and extracts the hyperlinks to the individual resumes.\n",
    "    It returns a list of all of the hyperlinks associated with a resume.\n",
    "    '''\n",
    "    driver = webdriver.Chrome('/Users/christopherluciuk/Downloads/chromedriver')\n",
    "    #The last concatenation lets us navigate through pages\n",
    "    driver.get('https://www.indeed.com/resumes?l=new+york&co=US&start='+str(start))\n",
    "    soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "    driver.close()\n",
    "    links = []\n",
    "    for item in soup.find_all(name='li',attrs={'data-tn-component':'resume-search-result'}):\n",
    "        for tag in item.find_all(name='a',attrs={'data-tn-element':'resume-result-link[]'}):\n",
    "            if tag['href'] is None:\n",
    "                pass\n",
    "            else:\n",
    "                links.append('https://www.indeed.com'+str(tag['href']))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_job_pairs(links):\n",
    "    '''\n",
    "    This function follows a link and returns a list of the pairs of jobs that appear adjacent\n",
    "    to each other in time. It returns all the job pairs for all of the links in a list.\n",
    "    '''\n",
    "    list_pairs = []\n",
    "    for url in links:\n",
    "        driver = webdriver.Chrome('/Users/christopherluciuk/Downloads/chromedriver')\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "        driver.close()\n",
    "        work_exp = []\n",
    "        for item in soup.find_all(name='div',attrs={'class':'work-experience-section'}):\n",
    "            for tag in item.find_all(name='p',attrs={'class':'work_title title'}):\n",
    "                if tag is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    work_exp.append(tag.text.strip())\n",
    "        #Now find the chronological pairs\n",
    "        for i in range(len(work_exp)-1):\n",
    "            pair = [work_exp[i],work_exp[i+1]]\n",
    "            list_pairs.append(pair)\n",
    "        time.sleep(120) #Timer to limit how much I scrape\n",
    "    return list_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = get_soup(start=100)\n",
    "data = get_job_pairs(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the job pairs to a postgreSQL database\n",
    "\n",
    "#Create or load database\n",
    "# Define a database name\n",
    "# Set your postgres username\n",
    "dbname = 'indeed_resumes_v1'\n",
    "username = 'christopherluciuk' # change this to your username\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "\n",
    "#Create\n",
    "#con = psycopg2.connect(database = dbname, user = username)\n",
    "#cur = con.cursor()\n",
    "#Comment out after having created the table\n",
    "#cur.execute(\"CREATE TABLE data_table(id INTEGER PRIMARY KEY, Title1 VARCHAR(500), Title2 VARCHAR(500))\")\n",
    "\n",
    "#con.commit()\n",
    "#Close\n",
    "#con.close()\n",
    "\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "cur = con.cursor()\n",
    "\n",
    "\n",
    "#How to add python array to database\n",
    "sql_query = '''SELECT * FROM data_table ORDER BY ID DESC LIMIT 1;'''\n",
    "data_out = pd.read_sql_query(sql_query,con)\n",
    "last_val = int(data_out['id'].values)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    item_index = i + (last_val+1) #Need to add this criteria when the database exists!\n",
    "    data[i].insert(0,item_index)\n",
    "    cur.execute(\"INSERT INTO data_table VALUES(%s, %s, %s)\", (data[i]))\n",
    "    \n",
    "con.commit()\n",
    "cur.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that I wrote the pairs I thought I did\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)\n",
    "sql_query = '''SELECT * FROM data_table;'''\n",
    "data_out = pd.read_sql_query(sql_query,con)\n",
    "con.close()\n",
    "display(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
